{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nimport blobfile as bf\\nimport transformer_lens\\nfrom sparse_model import Autoencoder\\nfrom tqdm import tqdm\\nimport pickle as pkl\\n\\ndevice = \"mps\"\\n\\n# Load the autoencoder\\nlayer_index = 0  # in range(12)\\nautoencoder_input = [\"mlp_post_act\", \"resid_delta_mlp\"][0]\\nfilename = f\"az://openaipublic/sparse-autoencoder/gpt2-small/{autoencoder_input}/autoencoders/{layer_index}.pt\"\\n\\nwith bf.BlobFile(filename, mode=\"rb\") as f:\\n    print(\"Inside the blobfile\")\\n    print()\\n    state_dict = torch.load(f)\\n    with open(\"state_dict.pkl\", \"wb\") as f:\\n        pkl.dump(state_dict, f)\\n    autoencoder = Autoencoder.from_state_dict(state_dict)\\n\\n\\n\\n# Extract neuron activations with transformer_lens\\nmodel = transformer_lens.HookedTransformer.from_pretrained(\"gpt2\", center_writing_weights=False)\\nprompt = \"This is an example of a prompt that\"\\nprint(prompt)\\nprint()\\ntokens = model.to_tokens(prompt)  # (1, n_tokens)\\nprint(model.to_str_tokens(tokens))\\nwith torch.no_grad():\\n    print(\"Inside grad\")\\n    logits, activation_cache = model.run_with_cache(tokens, remove_batch_dim=True)\\nif autoencoder_input == \"mlp_post_act\":\\n    input_tensor = activation_cache[f\"blocks.{layer_index}.mlp.hook_post\"]  # (n_tokens, n_neurons)\\nelif autoencoder_input == \"resid_delta_mlp\":\\n    input_tensor = activation_cache[f\"blocks.{layer_index}.hook_mlp_out\"]  # (n_tokens, n_residual_channels)\\n\\n# Encode neuron activations with the autoencoder\\ndevice = next(model.parameters()).device\\nautoencoder.to(device)\\nwith torch.no_grad():\\n    print(f\"The input tensor is {input_tensor}\")\\n    print()\\n    latent_activations = autoencoder.encode(input_tensor)  # (n_tokens, n_latents)\\n    print(f\"The latent activation is {latent_activations}\")\\n    print()\\n    original_activations = autoencoder.decode(latent_activations)  # (n_tokens, n_neurons)\\n    print(f\"The original activation is {original_activations}\")\\n    print()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "import blobfile as bf\n",
    "import transformer_lens\n",
    "from sparse_model import Autoencoder\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "# Load the autoencoder\n",
    "layer_index = 0  # in range(12)\n",
    "autoencoder_input = [\"mlp_post_act\", \"resid_delta_mlp\"][0]\n",
    "filename = f\"az://openaipublic/sparse-autoencoder/gpt2-small/{autoencoder_input}/autoencoders/{layer_index}.pt\"\n",
    "\n",
    "with bf.BlobFile(filename, mode=\"rb\") as f:\n",
    "    print(\"Inside the blobfile\")\n",
    "    print()\n",
    "    state_dict = torch.load(f)\n",
    "    with open(\"state_dict.pkl\", \"wb\") as f:\n",
    "        pkl.dump(state_dict, f)\n",
    "    autoencoder = Autoencoder.from_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Extract neuron activations with transformer_lens\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2\", center_writing_weights=False)\n",
    "prompt = \"This is an example of a prompt that\"\n",
    "print(prompt)\n",
    "print()\n",
    "tokens = model.to_tokens(prompt)  # (1, n_tokens)\n",
    "print(model.to_str_tokens(tokens))\n",
    "with torch.no_grad():\n",
    "    print(\"Inside grad\")\n",
    "    logits, activation_cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "if autoencoder_input == \"mlp_post_act\":\n",
    "    input_tensor = activation_cache[f\"blocks.{layer_index}.mlp.hook_post\"]  # (n_tokens, n_neurons)\n",
    "elif autoencoder_input == \"resid_delta_mlp\":\n",
    "    input_tensor = activation_cache[f\"blocks.{layer_index}.hook_mlp_out\"]  # (n_tokens, n_residual_channels)\n",
    "\n",
    "# Encode neuron activations with the autoencoder\n",
    "device = next(model.parameters()).device\n",
    "autoencoder.to(device)\n",
    "with torch.no_grad():\n",
    "    print(f\"The input tensor is {input_tensor}\")\n",
    "    print()\n",
    "    latent_activations = autoencoder.encode(input_tensor)  # (n_tokens, n_latents)\n",
    "    print(f\"The latent activation is {latent_activations}\")\n",
    "    print()\n",
    "    original_activations = autoencoder.decode(latent_activations)  # (n_tokens, n_neurons)\n",
    "    print(f\"The original activation is {original_activations}\")\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "import torch as t\n",
    "import einops\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from jaxtyping import Float\n",
    "from transformer_lens import ActivationCache\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import pprint\n",
    "import json \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "SAVE_DIR = Path(\"/workspace/1L-Sparse-Autoencoder/checkpoints\")\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        d_hidden = cfg[\"dict_size\"]\n",
    "        l1_coeff = cfg[\"l1_coeff\"]\n",
    "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(cfg[\"seed\"])\n",
    "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(cfg[\"act_size\"], d_hidden, dtype=dtype)))\n",
    "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, cfg[\"act_size\"], dtype=dtype)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(cfg[\"act_size\"], dtype=dtype))\n",
    "\n",
    "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "\n",
    "        self.to(cfg[\"device\"])\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x_cent = x - self.b_dec\n",
    "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
    "        print(acts.shape)\n",
    "        acts[:,-2,:] = acts[:,-2,:] * mask\n",
    "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
    "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
    "        loss = l2_loss + l1_loss\n",
    "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def make_decoder_weights_and_grad_unit_norm(self):\n",
    "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
    "        self.W_dec.grad -= W_dec_grad_proj\n",
    "        # Bugfix(?) for ensuring W_dec retains unit norm, this was not there when I trained my original autoencoders.\n",
    "        self.W_dec.data = W_dec_normed\n",
    "    \n",
    "    def get_version(self):\n",
    "        version_list = [int(file.name.split(\".\")[0]) for file in list(SAVE_DIR.iterdir()) if \"pt\" in str(file)]\n",
    "        if len(version_list):\n",
    "            return 1+max(version_list)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def save(self):\n",
    "        version = self.get_version()\n",
    "        torch.save(self.state_dict(), SAVE_DIR/(str(version)+\".pt\"))\n",
    "        with open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"w\") as f:\n",
    "            json.dump(cfg, f)\n",
    "        print(\"Saved as version\", version)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, version):\n",
    "        cfg = (json.load(open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"r\")))\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(torch.load(SAVE_DIR/(str(version)+\".pt\")))\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_hf(cls, version, device_override=None):\n",
    "        \"\"\"\n",
    "        Loads the saved autoencoder from HuggingFace. \n",
    "        \n",
    "        Version is expected to be an int, or \"run1\" or \"run2\"\n",
    "\n",
    "        version 25 is the final checkpoint of the first autoencoder run,\n",
    "        version 47 is the final checkpoint of the second autoencoder run.\n",
    "        \"\"\"\n",
    "        if version==\"run1\":\n",
    "            version = 25\n",
    "        elif version==\"run2\":\n",
    "            version = 47\n",
    "        \n",
    "        cfg = utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}_cfg.json\")\n",
    "        if device_override is not None:\n",
    "            cfg[\"device\"] = device_override\n",
    "\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}.pt\", force_is_torch=True))\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "# load gpt2-small\n",
    "# model = HookedTransformer.from_pretrained(\"gpt2-small\").to('mps')\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=DEVICE)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "point, layer = \"resid_pre\", 10\n",
    "dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    \"dict_size\": 6144,\n",
    "    \"act_size\": 768,\n",
    "    \"l1_coeff\": 0.001,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"seed\": 0,\n",
    "    \"device\": DEVICE,\n",
    "    \"model_batch_size\": 1028,\n",
    "}\n",
    "encoder_resid_pre_10 = AutoEncoder(cfg)\n",
    "encoder_resid_pre_10.load_state_dict(dic)\n",
    "\n",
    "point, layer = \"resid_pre\", 11\n",
    "dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    \"dict_size\": 6144,\n",
    "    \"act_size\": 768,\n",
    "    \"l1_coeff\": 0.001,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"seed\": 0,\n",
    "    \"device\": DEVICE,\n",
    "    \"model_batch_size\": 1028,\n",
    "}\n",
    "encoder_resid_pre_11 = AutoEncoder(cfg)\n",
    "encoder_resid_pre_11.load_state_dict(dic)\n",
    "\n",
    "data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "# tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "# tokenized_data = tokenized_data.shuffle(22)\n",
    "\n",
    "# from transformer_lens import utils\n",
    "\n",
    "example_prompt = \"After Jack and Mary went to the store, Jack gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "# utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = torch.load(\"gpt2-small-sparse-autoencoders/gpt2-small_6144_mlp_out_0.pt\", map_location=torch.device(DEVICE))\n",
    "\n",
    "cfg = {\n",
    "    \"dict_size\": 6144,\n",
    "    \"act_size\": 768,\n",
    "    \"l1_coeff\": 0.001,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"seed\": 0,\n",
    "    \"device\": DEVICE,\n",
    "    \"model_batch_size\": 1,\n",
    "}\n",
    "# encoder_resid_pre_10 = AutoEncoder(cfg)\n",
    "# encoder_resid_pre_10.load_state_dict(dic)\n",
    "encoder_mlp_out_0 = AutoEncoder(cfg)\n",
    "encoder_mlp_out_0.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10\n",
      "torch.Size([1, 10, 6144])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Paris'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The Eiffel Tower is in the city of\"\n",
    "a = model.tokenizer.encode(text, return_tensors = 'pt')\n",
    "a_len = model.tokenizer.tokenize(text)\n",
    "print(len(a))\n",
    "print(len(a_len))\n",
    "mask = torch.ones((1,1, 6144))\n",
    "\n",
    "with model.trace(a) as tracer:\n",
    "    acts = model.transformer.h[0].output[0]\n",
    "    # loss, x_reconstruct, acts, l2_loss, l1_loss = encoder_resid_pre_10(acts, mask)\n",
    "    loss, x_reconstruct, acts, l2_loss, l1_loss = encoder_mlp_out_0(acts, mask)\n",
    "    b = model.transformer.h[0].output.save()\n",
    "    model.transformer.h[0].output[0][:,:,:] = x_reconstruct\n",
    "    output = model.lm_head.output.argmax(dim = -1).save()\n",
    "\n",
    "model.tokenizer.decode(output[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2]*10, dtype = torch.float32)\n",
    "print(a)\n",
    "b = torch.ones((10,), dtype = torch.float32)\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
