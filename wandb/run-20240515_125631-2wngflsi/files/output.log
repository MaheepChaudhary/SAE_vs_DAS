  0%|          | 0/4 [00:00<?, ?it/s]  Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_53136/2715793180.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  embed_mask_sigmoid = t.sigmoid(self.embed_mask / t.tensor(self.temperature))
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_53136/2715793180.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  l1_mask_sigmoid = t.sigmoid(self.l1_mask / t.tensor(self.temperature))
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_53136/2715793180.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  l2_mask_sigmoid = t.sigmoid(self.l2_mask / t.tensor(self.temperature))
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_53136/2715793180.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  l3_mask_sigmoid = t.sigmoid(self.l3_mask / t.tensor(self.temperature))
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_53136/2715793180.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  l4_mask_sigmoid = t.sigmoid(self.l4_mask / t.tensor(self.temperature))
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100 [00:01<?, ?it/s]
  0%|          | 0/4 [00:01<?, ?it/s]
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(