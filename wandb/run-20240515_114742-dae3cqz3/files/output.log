/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
[{'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]
['The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog']
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
{'input_ids': [[101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
{'input_ids': [[101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102], [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True)
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)
  0%|          | 0/4 [00:00<?, ?it/s]  You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 0/100 [00:01<?, ?it/s]
  0%|          | 0/4 [00:01<?, ?it/s]
  0%|          | 0/100 [00:01<?, ?it/s]
  0%|          | 0/100 [00:00<?, ?it/s]
  0%|          | 0/4 [00:00<?, ?it/s]s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)cific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  self.l4_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l4_mask / t.tensor(self.temperature)), requires_grad=True)cific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True) from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  self.l3_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l3_mask / t.tensor(self.temperature)), requires_grad=True) from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/1740508012.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  self.l2_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l2_mask / t.tensor(self.temperature)), requires_grad=True) from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/585733749.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)..quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/585733749.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)..quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
The quick brown fox jumps over the lazy dog
<built-in method size of Tensor object at 0x41f9e1ef0>
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)..quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)..quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
The quick brown fox jumps over the lazy dog
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0',
       grad_fn=<ReluBackward0>)
  self.l1_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.l1_mask / t.tensor(self.temperature)), requires_grad=True)from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)..quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
The quick brown fox jumps over the lazy dog
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/3254406591.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_10411/3254406591.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
The quick brown fox jumps over the lazy dog
  self.embed_mask_sigmoid = t.nn.Parameter(t.sigmoid(self.embed_mask / t.tensor(self.temperature)), requires_grad=True)a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).quences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.h the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.h the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.