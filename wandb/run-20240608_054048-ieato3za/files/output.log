
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Traceback (most recent call last):
  File "/home/atticus/maheep/SAE_vs_DAS/main.py", line 314, in <module>
    eval(args.device,
  File "/home/atticus/maheep/SAE_vs_DAS/main.py", line 136, in eval
    new_model.load_state_dict(t.load(saved_model_path))
  File "/home/atticus/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for my_model:
	size mismatch for l4_mask: copying a param with shape torch.Size([1, 1, 32768]) from checkpoint, the shape in current model is torch.Size([1, 1, 512]).
Traceback (most recent call last):
  File "/home/atticus/maheep/SAE_vs_DAS/main.py", line 314, in <module>
    eval(args.device,
  File "/home/atticus/maheep/SAE_vs_DAS/main.py", line 136, in eval
    new_model.load_state_dict(t.load(saved_model_path))
  File "/home/atticus/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for my_model:
	size mismatch for l4_mask: copying a param with shape torch.Size([1, 1, 32768]) from checkpoint, the shape in current model is torch.Size([1, 1, 512]).