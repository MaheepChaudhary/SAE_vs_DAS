
passed dict emned path /Users/maheepchaudhary/pytorch/Projects/concept_eraser_research/DAS_MAT/baulab.us/u/smarks/autoencoders/pythia-70m-deduped/embed
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                                                                                                        | 0/1000 [00:00<?, ?it/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Epoch: 0, Loss: 27.334432220458986
  2%|██▏                                                                                                                            | 17/1000 [00:02<01:54,  8.57it/s]
Epoch: 0, Loss: 97.2213303565979

  4%|████▍                                                                                                                          | 35/1000 [00:04<01:51,  8.65it/s]
Epoch: 0, Loss: 34.926334857940674


  6%|███████▋                                                                                                                       | 61/1000 [00:08<02:35,  6.04it/s]

  8%|█████████▌                                                                                                                     | 75/1000 [00:10<02:23,  6.43it/s]
Epoch: 0, Loss: 58.9636344909668

  9%|███████████▏                                                                                                                   | 88/1000 [00:11<01:25, 10.69it/s]
Epoch: 0, Loss: 149.98520331382753


 13%|████████████████▍                                                                                                             | 130/1000 [00:16<01:26, 10.00it/s]
Epoch: 0, Loss: 43.08399848937988
Epoch: 0, Loss: 8.676067733764649

 14%|██████████████████▎                                                                                                           | 145/1000 [00:17<01:15, 11.32it/s]

 16%|███████████████████▉                                                                                                          | 158/1000 [00:20<02:43,  5.16it/s]

 18%|██████████████████████▎                                                                                                       | 177/1000 [00:22<01:45,  7.84it/s]
Epoch: 0, Loss: 34.91699674129486

 20%|████████████████████████▌                                                                                                     | 195/1000 [00:24<01:28,  9.14it/s]
Epoch: 0, Loss: 57.95402946472168
Epoch: 0, Loss: 48.374492573738095
Epoch: 0, Loss: 40.22955446243286

 22%|███████████████████████████▏                                                                                                  | 216/1000 [00:26<01:18,  9.96it/s]
Epoch: 0, Loss: 135.92554893493653

 23%|█████████████████████████████▍                                                                                                | 234/1000 [00:28<01:14, 10.26it/s]
Epoch: 0, Loss: 23.89782295227051

 26%|████████████████████████████████▎                                                                                             | 256/1000 [00:30<01:19,  9.31it/s]
Epoch: 0, Loss: 80.39093017578125

 28%|███████████████████████████████████                                                                                           | 278/1000 [00:32<01:02, 11.61it/s]
Epoch: 0, Loss: 26.191478538513184
Epoch: 0, Loss: 58.370886993408206

 30%|██████████████████████████████████████▎                                                                                       | 304/1000 [00:34<00:42, 16.19it/s]
Epoch: 0, Loss: 23.514972201688217


 35%|███████████████████████████████████████████▌                                                                                  | 346/1000 [00:38<01:15,  8.63it/s]
Epoch: 0, Loss: 60.870123863220215

 37%|██████████████████████████████████████████████▊                                                                               | 372/1000 [00:40<00:42, 14.85it/s]
Epoch: 0, Loss: 19.62917003631592
Epoch: 0, Loss: 141.91501938104616

 40%|█████████████████████████████████████████████████▉                                                                            | 396/1000 [00:42<00:42, 14.18it/s]
Epoch: 0, Loss: 19.501455116271973

 42%|█████████████████████████████████████████████████████▏                                                                        | 422/1000 [00:44<00:44, 13.10it/s]
Epoch: 0, Loss: 74.87294731140136
Epoch: 0, Loss: 29.43608033657074
Epoch: 0, Loss: 36.68002471923828
Epoch: 0, Loss: 29.274617385864257

 45%|████████████████████████████████████████████████████████▏                                                                     | 446/1000 [00:46<00:44, 12.34it/s]
Epoch: 0, Loss: 53.62791242599487

 46%|█████████████████████████████████████████████████████████▋                                                                    | 458/1000 [00:47<01:11,  7.57it/s]
Epoch: 0, Loss: 29.741948318481445

 48%|████████████████████████████████████████████████████████████▏                                                                 | 478/1000 [00:50<00:45, 11.59it/s]

 49%|██████████████████████████████████████████████████████████████▏                                                               | 494/1000 [00:51<00:50, 10.10it/s]
Epoch: 0, Loss: 37.043843841552736
Epoch: 0, Loss: 58.70074615478516

 51%|████████████████████████████████████████████████████████████████▊                                                             | 514/1000 [00:54<00:35, 13.61it/s]
Epoch: 0, Loss: 34.9720458984375


 56%|██████████████████████████████████████████████████████████████████████▍                                                       | 559/1000 [00:58<00:29, 14.81it/s]
Epoch: 0, Loss: 75.50796737670899

 59%|██████████████████████████████████████████████████████████████████████████▏                                                   | 589/1000 [01:00<00:31, 13.16it/s]
Epoch: 0, Loss: 11.725484466552734

 60%|████████████████████████████████████████████████████████████████████████████                                                  | 604/1000 [01:02<00:51,  7.65it/s]
Epoch: 0, Loss: 62.424908828735354

 62%|██████████████████████████████████████████████████████████████████████████████▎                                               | 622/1000 [01:04<00:41,  9.17it/s]
Epoch: 0, Loss: 72.99740962982177

 64%|████████████████████████████████████████████████████████████████████████████████▊                                             | 641/1000 [01:06<00:39,  9.10it/s]
Epoch: 0, Loss: 45.93762969970703

 66%|███████████████████████████████████████████████████████████████████████████████████▎                                          | 661/1000 [01:08<00:28, 11.96it/s]
Epoch: 0, Loss: 39.88295574188233

 68%|█████████████████████████████████████████████████████████████████████████████████████▌                                        | 679/1000 [01:10<00:34,  9.31it/s]

 70%|███████████████████████████████████████████████████████████████████████████████████████▉                                      | 698/1000 [01:12<00:29, 10.09it/s]
Epoch: 0, Loss: 20.95863389968872

 72%|██████████████████████████████████████████████████████████████████████████████████████████▏                                   | 716/1000 [01:14<00:22, 12.79it/s]
Epoch: 0, Loss: 94.19110298156738

 74%|████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 735/1000 [01:16<00:29,  9.11it/s]
Epoch: 0, Loss: 79.28313908576965

 75%|██████████████████████████████████████████████████████████████████████████████████████████████▉                               | 753/1000 [01:18<00:25,  9.82it/s]
Epoch: 0, Loss: 89.97323962297523

 78%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 779/1000 [01:20<00:18, 12.01it/s]
Epoch: 0, Loss: 21.211685180664062

 80%|████████████████████████████████████████████████████████████████████████████████████████████████████▍                         | 797/1000 [01:22<00:27,  7.41it/s]
Epoch: 0, Loss: 73.06710948944092

 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                       | 816/1000 [01:24<00:13, 13.19it/s]
Epoch: 0, Loss: 122.53301488161087

 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 836/1000 [01:26<00:16,  9.76it/s]
Epoch: 0, Loss: 38.490391325950625

 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 866/1000 [01:28<00:08, 15.55it/s]
Epoch: 0, Loss: 59.66762037277222
Epoch: 0, Loss: 52.63732175827026

 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 890/1000 [01:30<00:12,  8.99it/s]
Epoch: 0, Loss: 81.65840606689453

 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏         | 922/1000 [01:32<00:05, 15.16it/s]
Epoch: 0, Loss: 34.448677444458006
Epoch: 0, Loss: 0.0004730937071144581
Epoch: 0, Loss: 67.19445106934727

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 956/1000 [01:34<00:02, 16.94it/s]
Epoch: 0, Loss: 43.54027099609375
Epoch: 0, Loss: 41.092771410838395

 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 990/1000 [01:36<00:00, 16.91it/s]
Epoch: 0, Loss: 37.347448432445525
Epoch: 0, Loss: 48.95263595581055
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:37<00:00, 10.28it/s]
  2%|██▊                                                                                                                            | 22/1000 [00:01<00:58, 16.78it/s]
Epoch: 0, Loss: 27.720233535766603
Epoch: 0, Loss: 51.75701751708984
Epoch: 1, Loss: 27.334432220458986
  5%|██████▏                                                                                                                        | 49/1000 [00:03<01:05, 14.52it/s]
Traceback (most recent call last):
  File "/Users/maheepchaudhary/pytorch/Projects/SAE_vs_DAS/main.py", line 256, in <module>
    train(DEVICE=args.device,
  File "/Users/maheepchaudhary/pytorch/Projects/SAE_vs_DAS/main.py", line 81, in train
    logits = new_model(text[0], temperature=temprature)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/pytorch/Projects/SAE_vs_DAS/model.py", line 122, in forward
    with self.model.trace(text) as tracer:
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/contexts/Runner.py", line 49, in __exit__
    super().__exit__(exc_type, exc_val, exc_tb)
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/contexts/Tracer.py", line 69, in __exit__
    output = self._model.interleave(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/models/NNsightModel.py", line 245, in interleave
    with HookHandler(
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/intervention.py", line 450, in __exit__
    raise exc_val
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/models/NNsightModel.py", line 255, in interleave
    output = fn(*inputs, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/models/mixins/Generation.py", line 21, in _execute
    return self._execute_forward(prepared_inputs, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/models/LanguageModel.py", line 281, in _execute_forward
    return self._model(
           ^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 1048, in forward
    lm_logits = self.embed_out(hidden_states)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: MPS backend out of memory (MPS allocated: 20.36 GB, other allocations: 32.92 MB, max allowed: 20.40 GB). Tried to allocate 22.84 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
Epoch: 1, Loss: 68.81929292678834
Epoch: 1, Loss: 34.926334857940674