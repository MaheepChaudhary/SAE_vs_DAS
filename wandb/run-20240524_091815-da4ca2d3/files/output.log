
passed dict emned path /Users/maheepchaudhary/pytorch/Projects/concept_eraser_research/DAS_MAT/baulab.us/u/smarks/autoencoders/pythia-70m-deduped/embed
/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                                                                                                        | 0/1000 [00:00<?, ?it/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  1%|█▏                                                                                                                              | 9/1000 [00:02<03:01,  5.45it/s]
Epoch: 0, Loss: 51.19103660583496


  4%|█████                                                                                                                          | 40/1000 [00:06<02:11,  7.31it/s]
Epoch: 0, Loss: 11.251859718561173

  6%|███████▏                                                                                                                       | 57/1000 [00:08<01:42,  9.23it/s]
Epoch: 0, Loss: 47.921577548980714
Epoch: 0, Loss: 23.507557106018066

  7%|█████████                                                                                                                      | 71/1000 [00:10<01:28, 10.45it/s]

  9%|███████████                                                                                                                    | 87/1000 [00:12<01:34,  9.63it/s]
Epoch: 0, Loss: 49.480288345366716
Epoch: 0, Loss: 0.0007784624584019184

 11%|█████████████▉                                                                                                                | 111/1000 [00:14<01:30,  9.84it/s]
Epoch: 0, Loss: 11.332122421264648

 14%|█████████████████                                                                                                             | 135/1000 [00:16<01:14, 11.63it/s]
Epoch: 0, Loss: 4.419169235229492


 17%|█████████████████████▉                                                                                                        | 174/1000 [00:20<02:09,  6.38it/s]
Epoch: 0, Loss: 72.89394111633301
Epoch: 0, Loss: 68.92963278293603
Epoch: 0, Loss: 27.686603164672853

 19%|███████████████████████▉                                                                                                      | 190/1000 [00:22<01:19, 10.16it/s]
Epoch: 0, Loss: 70.19725036621094

 21%|██████████████████████████▋                                                                                                   | 212/1000 [00:24<01:19,  9.87it/s]

 22%|████████████████████████████▎                                                                                                 | 225/1000 [00:26<02:01,  6.38it/s]
Epoch: 0, Loss: 41.72398071289062

 24%|██████████████████████████████▊                                                                                               | 245/1000 [00:28<01:20,  9.38it/s]
Epoch: 0, Loss: 17.298799324035645

 27%|█████████████████████████████████▋                                                                                            | 267/1000 [00:30<00:56, 13.00it/s]
Epoch: 0, Loss: 22.871043395996093
Epoch: 0, Loss: 49.79417998790741

 29%|████████████████████████████████████▋                                                                                         | 291/1000 [00:32<00:58, 12.14it/s]
Epoch: 0, Loss: 32.84537539482117

 31%|███████████████████████████████████████▍                                                                                      | 313/1000 [00:34<01:14,  9.24it/s]
Epoch: 0, Loss: 32.01889934539795
Epoch: 0, Loss: 52.79231298305094


 36%|█████████████████████████████████████████████▉                                                                                | 365/1000 [00:38<00:42, 14.95it/s]
Epoch: 0, Loss: 17.873826122283937

 39%|█████████████████████████████████████████████████                                                                             | 389/1000 [00:40<01:06,  9.24it/s]
Epoch: 0, Loss: 29.127854776382446

 41%|███████████████████████████████████████████████████▊                                                                          | 411/1000 [00:42<00:43, 13.59it/s]
Epoch: 0, Loss: 53.27009735107422
Epoch: 0, Loss: 34.975323867797854

 44%|██████████████████████████████████████████████████████▊                                                                       | 435/1000 [00:44<00:40, 14.07it/s]
Epoch: 0, Loss: 7.075378799438477

 47%|██████████████████████████████████████████████████████████▊                                                                   | 467/1000 [00:46<00:32, 16.56it/s]
Epoch: 0, Loss: 32.54574394226074
Epoch: 0, Loss: 45.8669252753607

 49%|█████████████████████████████████████████████████████████████▌                                                                | 489/1000 [00:48<00:47, 10.75it/s]
Epoch: 0, Loss: 37.764796829223634
Epoch: 0, Loss: 43.62156524658203

 52%|█████████████████████████████████████████████████████████████████▏                                                            | 517/1000 [00:51<00:30, 15.81it/s]
Epoch: 0, Loss: 50.560191488265886

 55%|████████████████████████████████████████████████████████████████████▋                                                         | 545/1000 [00:53<00:34, 13.38it/s]
Epoch: 0, Loss: 60.48182220458985
Epoch: 0, Loss: 49.99810215234756

 56%|███████████████████████████████████████████████████████████████████████▏                                                      | 565/1000 [00:54<00:28, 15.10it/s]
Epoch: 0, Loss: 35.04000940322876

 59%|██████████████████████████████████████████████████████████████████████████▋                                                   | 593/1000 [00:56<00:37, 10.80it/s]
Epoch: 0, Loss: 72.71222305297852
Epoch: 0, Loss: 28.86177053451538
Epoch: 0, Loss: 51.36126861572266
Epoch: 0, Loss: 7.3688812255859375
Epoch: 0, Loss: 70.68772163391114

 62%|█████████████████████████████████████████████████████████████████████████████▋                                                | 617/1000 [00:58<00:30, 12.48it/s]


 67%|████████████████████████████████████████████████████████████████████████████████████▍                                         | 670/1000 [01:02<00:21, 15.50it/s]
Epoch: 0, Loss: 16.801215324923398
Epoch: 0, Loss: 30.251730155944824

 70%|███████████████████████████████████████████████████████████████████████████████████████▋                                      | 696/1000 [01:04<00:23, 12.76it/s]
Epoch: 0, Loss: 22.36352744102478
Epoch: 0, Loss: 17.570719146728514

 70%|███████████████████████████████████████████████████████████████████████████████████████▉                                      | 698/1000 [01:04<00:23, 12.98it/s]

 71%|█████████████████████████████████████████████████████████████████████████████████████████▍                                    | 710/1000 [01:06<00:33,  8.54it/s]




 72%|██████████████████████████████████████████████████████████████████████████████████████████▉                                   | 722/1000 [01:36<01:54,  2.44it/s]

 75%|█████████████████████████████████████████████████████████████████████████████████████████████▉                                | 746/1000 [01:38<00:17, 14.24it/s]
Epoch: 0, Loss: 88.56730880737305

 77%|█████████████████████████████████████████████████████████████████████████████████████████████████                             | 770/1000 [01:40<00:15, 15.27it/s]
Epoch: 0, Loss: 47.67078168392182

 80%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                         | 800/1000 [01:42<00:13, 14.95it/s]
Epoch: 0, Loss: 55.501476001739505
Epoch: 0, Loss: 32.75086050033569

 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉                        | 809/1000 [01:44<00:27,  7.03it/s]
Epoch: 0, Loss: 83.71523389816284

 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 833/1000 [01:46<00:11, 14.58it/s]
Epoch: 0, Loss: 16.75517439842224

 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋                  | 855/1000 [01:48<00:15,  9.38it/s]
Epoch: 0, Loss: 18.37085003852844

 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊               | 879/1000 [01:50<00:13,  9.18it/s]
Epoch: 0, Loss: 42.53438000679016
Epoch: 0, Loss: 20.396564865112303
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 885/1000 [01:51<00:14,  7.97it/s]
Traceback (most recent call last):
  File "/Users/maheepchaudhary/pytorch/Projects/SAE_vs_DAS/main.py", line 247, in <module>
    train(DEVICE=args.device,
  File "/Users/maheepchaudhary/pytorch/Projects/SAE_vs_DAS/main.py", line 81, in train
    logits = new_model(text[0], temperature=temprature)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/pytorch/Projects/SAE_vs_DAS/model.py", line 122, in forward
    with self.model.trace(text) as tracer:
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/contexts/Runner.py", line 49, in __exit__
    super().__exit__(exc_type, exc_val, exc_tb)
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/contexts/Tracer.py", line 69, in __exit__
    output = self._model.interleave(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/models/NNsightModel.py", line 245, in interleave
    with HookHandler(
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/intervention.py", line 450, in __exit__
    raise exc_val
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/models/NNsightModel.py", line 255, in interleave
    output = fn(*inputs, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/models/mixins/Generation.py", line 21, in _execute
    return self._execute_forward(prepared_inputs, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/nnsight/models/LanguageModel.py", line 281, in _execute_forward
    return self._model(
           ^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 1048, in forward
    lm_logits = self.embed_out(hidden_states)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: MPS backend out of memory (MPS allocated: 20.36 GB, other allocations: 32.89 MB, max allowed: 20.40 GB). Tried to allocate 16.12 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).