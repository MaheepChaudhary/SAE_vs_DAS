{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(14893) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Users/maheepchaudhary/miniforge3/envs/nnsight/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "python(14918) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(14946) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(14973) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from nnsight import LanguageModel \n",
    "import torch as t\n",
    "from torch import nn\n",
    "# from attribution import patching_effect\n",
    "from dictionary_learning import AutoEncoder, ActivationBuffer\n",
    "# from dictionary_learning.dictionary import IdentityDict\n",
    "# from dictionary_learning.interp import examine_dimension\n",
    "# from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "DEBUGGING = False\n",
    "\n",
    "if DEBUGGING:\n",
    "    tracer_kwargs = dict(scan=True, validate=True)\n",
    "else:\n",
    "    tracer_kwargs = dict(scan=False, validate=False)\n",
    "\n",
    "# model hyperparameters\n",
    "DEVICE = 'mps'\n",
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "activation_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "class Probe(nn.Module):\n",
    "    def __init__(self, activation_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(activation_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "with open(\"probe_shift.pkl\", \"rb\") as f:\n",
    "    probe = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(14980) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(15023) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(15048) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(15074) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(15101) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# loading dictionaries\n",
    "\n",
    "# dictionary hyperparameters\n",
    "dict_id = 10\n",
    "expansion_factor = 64\n",
    "dictionary_size = expansion_factor * activation_dim\n",
    "layer = 4\n",
    "\n",
    "submodules = []\n",
    "dictionaries = {}\n",
    "\n",
    "submodules.append(model.gpt_neox.embed_in)\n",
    "dictionaries[model.gpt_neox.embed_in] = AutoEncoder.from_pretrained(\n",
    "    f'/Users/maheepchaudhary/pytorch/Projects/concept_eraser_research/DAS_MAT/baulab.us/u/smarks/autoencoders/pythia-70m-deduped/embed/{dict_id}_{dictionary_size}/ae.pt',\n",
    "    device=DEVICE\n",
    ")\n",
    "for i in range(layer + 1):\n",
    "    submodules.append(model.gpt_neox.layers[i].attention)\n",
    "    dictionaries[model.gpt_neox.layers[i].attention] = AutoEncoder.from_pretrained(\n",
    "        f'/Users/maheepchaudhary/pytorch/Projects/concept_eraser_research/DAS_MAT/baulab.us/u/smarks/autoencoders/pythia-70m-deduped/attn_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i].mlp)\n",
    "    dictionaries[model.gpt_neox.layers[i].mlp] = AutoEncoder.from_pretrained(\n",
    "        f'/Users/maheepchaudhary/pytorch/Projects/concept_eraser_research/DAS_MAT/baulab.us/u/smarks/autoencoders/pythia-70m-deduped/mlp_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    submodules.append(model.gpt_neox.layers[i])\n",
    "    dictionaries[model.gpt_neox.layers[i]] = AutoEncoder.from_pretrained(\n",
    "        f'/Users/maheepchaudhary/pytorch/Projects/concept_eraser_research/DAS_MAT/baulab.us/u/smarks/autoencoders/pythia-70m-deduped/resid_out_layer{i}/{dict_id}_{dictionary_size}/ae.pt',\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "# metric fn is used to \n",
    "def metric_fn(model, labels=None):\n",
    "    attn_mask = model.input[1]['attention_mask']\n",
    "    acts = model.gpt_neox.layers[layer].output[0]\n",
    "    acts = acts * attn_mask[:, :, None]\n",
    "    acts = acts.sum(1) / attn_mask.sum(1)[:, None]\n",
    "    \n",
    "    return t.where(\n",
    "        labels == 0,\n",
    "        probe(acts),\n",
    "        - probe(acts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "text = \"\"\"The quick brown fox jumps over the lazy dog\"\"\"\n",
    "\n",
    "\n",
    "'''\n",
    "We make a dummy model to see if gradient descent works on the model.\n",
    "We will optimize the model to output the zero vector as activation in the end. \n",
    "\n",
    "After that we will analyse the values of each l1, l2, l3, l4 \n",
    "to see if the model has learned the values to manipulate the activations of the model.\n",
    "'''\n",
    "\n",
    "class SigmoidMaskIntervention(nn.Module):\n",
    "\n",
    "    \"\"\"Intervention in the original basis with binary mask.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mask = t.nn.Parameter(\n",
    "            t.zeros(embed_dim), requires_grad=True)\n",
    "        \n",
    "        self.temperature = t.nn.Parameter(t.tensor(0.01))\n",
    "\n",
    "    def get_temperature(self):\n",
    "        return self.temperature\n",
    "\n",
    "    def set_temperature(self, temp: t.Tensor):\n",
    "        self.temperature.data = temp\n",
    "\n",
    "    def forward(self, base, subspaces=None):\n",
    "        batch_size = base.shape[0]\n",
    "        # get boundary mask between 0 and 1 from sigmoid\n",
    "        mask_sigmoid = t.sigmoid(self.mask / t.tensor(self.temperature)) \n",
    "        \n",
    "        # interchange\n",
    "        # intervened_output = (\n",
    "        #     1.0 - mask_sigmoid\n",
    "        # ) * base + mask_sigmoid * source\n",
    "        '''I have changed the intervention to be only done on the base'''\n",
    "        intervened_output = mask_sigmoid * base\n",
    "\n",
    "        return intervened_output\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"SigmoidMaskIntervention()\"\n",
    "    \n",
    "\n",
    "class my_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(my_model, self).__init__()\n",
    "        \n",
    "        # We have intergrated the sigmoid_mask from pyvene (https://github.com/stanfordnlp/pyvene/blob/main/pyvene/models/interventions.py) \n",
    "        \n",
    "        embed_dim = (9,32768)\n",
    "        self.temperature = t.nn.Parameter(t.tensor(0.01))\n",
    "        \n",
    "        self.embed_mask = t.nn.Parameter(t.ones(embed_dim), requires_grad=True)\n",
    "        self.embed_mask_sigmoid = t.sigmoid(self.embed_mask / t.tensor(self.temperature)) \n",
    "        \n",
    "        self.l1_mask = t.nn.Parameter(t.ones(embed_dim), requires_grad=True)\n",
    "        self.l1_mask_sigmoid = t.sigmoid(self.l1_mask / t.tensor(self.temperature))\n",
    "        \n",
    "        self.l2_mask = t.nn.Parameter(t.ones(embed_dim), requires_grad=True)\n",
    "        self.l2_mask_sigmoid = t.sigmoid(self.l2_mask / t.tensor(self.temperature))\n",
    "        \n",
    "        self.l3_mask = t.nn.Parameter(t.ones(embed_dim), requires_grad=True)\n",
    "        self.l3_mask_sigmoid = t.sigmoid(self.l3_mask / t.tensor(self.temperature))\n",
    "        \n",
    "        self.l4_mask = t.nn.Parameter(t.ones(embed_dim), requires_grad=True)\n",
    "        self.l4_mask_sigmoid = t.sigmoid(self.l4_mask / t.tensor(self.temperature))\n",
    "        \n",
    "        # self.probe = Probe\n",
    "        \n",
    "    def forward(self,text):\n",
    "        \n",
    "        acts = self.get_acts(text, 0, 'None', 1)\n",
    "        acts = self.embed_mask * acts\n",
    "        acts = self.get_acts(text, 1, acts, 2)\n",
    "        acts = self.l1_mask * acts\n",
    "        acts = self.get_acts(text, 2, acts, 3)\n",
    "        acts = self.l2_mask * acts\n",
    "        acts = self.get_acts(text, 3, acts, 4)\n",
    "        acts = self.l3_mask * acts\n",
    "        acts = self.get_acts(text, 4, acts, 5)\n",
    "        acts = self.l4_mask * acts\n",
    "        acts = self.get_acts(text, 5, acts, 6)\n",
    "        # acts = self.probe(acts)\n",
    "    \n",
    "        return acts\n",
    "\n",
    "    '''\n",
    "    The get_acts function is used to get the activations of the model at a particular layer \n",
    "    after getting intervened at a particular layer.\n",
    "    '''\n",
    "    \n",
    "    def get_acts(self, text, intervention_layer, acts, get_act_layer):\n",
    "        with model.trace(text):\n",
    "            i = 0\n",
    "            for module in submodules:\n",
    "                \n",
    "                if type(module.output.shape) != tuple:\n",
    "                    \n",
    "                    if acts == 'None':\n",
    "                        new_acts = module.output[0].save()\n",
    "                        dictionary = dictionaries[module]\n",
    "                        new_acts = dictionary.encode(new_acts).save()\n",
    "                    \n",
    "                    elif get_act_layer == 6:\n",
    "                        new_acts = dictionaries[module].decode(acts)\n",
    "                    \n",
    "                    else:    \n",
    "                        if i == intervention_layer:\n",
    "                            dictionary = dictionaries[module]\n",
    "                            acts = dictionary.decode(acts)\n",
    "                            module.output[0] = acts\n",
    "                        elif i == get_act_layer:\n",
    "                            new_acts = module.output[0]\n",
    "                            new_acts = dictionaries[module].encode(new_acts).save()\n",
    "                        \n",
    "                    \n",
    "                    i+=1\n",
    "        return new_acts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aim is to find out if the model.trace also computes gradient and update the weights during backprop. \n",
    "\n",
    "class dummy_model(nn.Module):\n",
    "    def __init__(self, model, submodules, dictionaries):\n",
    "        super(self, dummy_model).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.dictionaries = dictionaries\n",
    "        self.submodules = submodules\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchaudhary-maheep28\u001b[0m (\u001b[33mcounterfactuals\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/maheepchaudhary/pytorch/Projects/concept_eraser_research/DAS_MAT/wandb/run-20240515_112811-sam6n97t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/counterfactuals/sae_concept_eraser/runs/sam6n97t' target=\"_blank\">quiet-pyramid-1</a></strong> to <a href='https://wandb.ai/counterfactuals/sae_concept_eraser' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/counterfactuals/sae_concept_eraser' target=\"_blank\">https://wandb.ai/counterfactuals/sae_concept_eraser</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/counterfactuals/sae_concept_eraser/runs/sam6n97t' target=\"_blank\">https://wandb.ai/counterfactuals/sae_concept_eraser/runs/sam6n97t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_92040/419729486.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embed_mask_sigmoid = t.sigmoid(self.embed_mask / t.tensor(self.temperature))\n",
      "/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_92040/419729486.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.l1_mask_sigmoid = t.sigmoid(self.l1_mask / t.tensor(self.temperature))\n",
      "/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_92040/419729486.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.l2_mask_sigmoid = t.sigmoid(self.l2_mask / t.tensor(self.temperature))\n",
      "/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_92040/419729486.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.l3_mask_sigmoid = t.sigmoid(self.l3_mask / t.tensor(self.temperature))\n",
      "/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_92040/419729486.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.l4_mask_sigmoid = t.sigmoid(self.l4_mask / t.tensor(self.temperature))\n",
      "100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      " 25%|██▌       | 1/4 [01:14<03:42, 74.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.10332609713077545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.97it/s]\n",
      " 50%|█████     | 2/4 [02:04<02:00, 60.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.015482081100344658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:48<00:00,  2.07it/s]\n",
      " 75%|███████▌  | 3/4 [02:53<00:54, 54.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.008295994251966476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:47<00:00,  2.09it/s]\n",
      "100%|██████████| 4/4 [03:41<00:00, 55.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.005572018679231405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here we will define the optimizer and all the things required to train the model.\n",
    "\n",
    "As for the data, we will use the text repeated many times as the data.\n",
    "'''\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"sae_concept_eraser\")\n",
    "\n",
    "def data_processing(text):\n",
    "    data = [text]*100\n",
    "    target = t.zeros(100,9,512)\n",
    "    return [data, target]\n",
    "\n",
    "\n",
    "new_model = my_model().to(DEVICE)\n",
    "\n",
    "optimizer = t.optim.Adam(new_model.parameters(), lr=0.01)\n",
    "epochs = 4\n",
    "criterion = nn.MSELoss().to(DEVICE)\n",
    "\n",
    "text = \"\"\"The quick brown fox jumps over the lazy dog\"\"\"\n",
    "\n",
    "data, target = data_processing(text)\n",
    "target = target.to(DEVICE)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i in tqdm(range(len(data))):\n",
    "        optimizer.zero_grad()\n",
    "        predicted = new_model(data[i])\n",
    "        loss = criterion(predicted, target[i])\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "    wandb.log({\"Epochs\": epoch, \"Loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = t.save(new_model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To compare the weights of the model with the original model.\n",
    "- We will compare the weights of the model with the weights of \n",
    "submodules and all. \n",
    "- One of the other things that we could do is compare the weights of dictionary with \n",
    "the initial dictionary weights. \n",
    "- Finally, if these 2 things gets fulfilled, we can compare the weights of the model\n",
    "with the weights of the original model.\n",
    "\n",
    "\n",
    "Based on Dr. Geiger text, I will just assume that the weights of the model will remain same and \n",
    "will start building the whole model for gender prediction. \n",
    "\n",
    "#TODO: Integrate mask and probe into the model. \n",
    "#TODO: Run and train the model on the gender dataset. \n",
    "'''\n",
    "\n",
    "def compare_weights(model, submodules):\n",
    "    initial_state = model.gpt_neox.embed_in.state_dict()\n",
    "    for module in submodules:\n",
    "        trained_state = module.state_dict()\n",
    "        # print(f\"Initial weights for {module}: {initial_state}\")\n",
    "        # print(f\"Trained weights for {module}: {trained_state}\")\n",
    "        if initial_state != trained_state:\n",
    "            print(f\"Weight for module has been updated!\")\n",
    "            print(initial_state)\n",
    "            print()\n",
    "            print(trained_state)\n",
    "        else:\n",
    "            print(f\"Weight for module remains unchanged\")\n",
    "        break\n",
    "\n",
    "compare_weights(new_model, submodules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model.embed_mask_sigmoid)\n",
    "print(new_model.l1_mask_sigmoid)\n",
    "print(new_model.l2_mask_sigmoid)\n",
    "print(new_model.l3_mask_sigmoid)\n",
    "print(new_model.l4_mask_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.gpt_neox.embed_in.weight)\n",
    "print()\n",
    "print(submodules[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = 'cpu'\n",
    "lm = LanguageModel('EleutherAI/pythia-70m-deduped', device_map=DEVICE, dispatch=True)\n",
    "lm.gpt_neox.embed_in.weight  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
